{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 选择适合任务的 CodeBERT 变体\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 示例源代码\n",
    "code = [\"def hello_world():\\n    print('Hello, World!')\",\"def hello_world():\\n    print('Hello, World!')\"]\n",
    "\n",
    "# 使用分词器编码源代码\n",
    "tokenized_code = tokenizer.encode_plus(code, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "# 获取模型输出\n",
    "with torch.no_grad():\n",
    "    model_output = model(**tokenized_code)\n",
    "\n",
    "# 提取模型输出的表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  9232, 20760,  1215,  8331, 49536, 50118,  1437,  1437,  1437,\n",
      "          5780, 45803, 31414,     6,   623,   328, 27645,     2,     2,  9232,\n",
      "         20760,  1215,  8331, 49536, 50118,  1437,  1437,  1437,  5780, 45803,\n",
      "         31414,     6,   623,   328, 27645,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name,param)\n",
    "print(tokenized_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class FreeLB(object):\n",
    "    def __init__(self, adv_K=3, adv_lr=1e-2, adv_init_mag=2e-2, adv_max_norm=0., adv_norm_type='l2'):\n",
    "        self.adv_K = adv_K\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_max_norm = adv_max_norm\n",
    "        self.adv_init_mag = adv_init_mag    # adv-training initialize with what magnitude, 即我们用多大的数值初始化delta\n",
    "        self.adv_norm_type = adv_norm_type\n",
    "        \n",
    "    def attack(self, model, inputs,labels, gradient_accumulation_steps=1):\n",
    "        input_ids = inputs['input_ids']\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            embeds_init = model.encoder.roberta.embeddings.word_embeddings(input_ids)\n",
    "        else:\n",
    "            embeds_init = model.encoder.roberta.embeddings.word_embeddings(input_ids)\n",
    "        if self.adv_init_mag > 0:   # 影响attack首步是基于原始梯度(delta=0)，还是对抗梯度(delta!=0)\n",
    "            input_mask = inputs['attention_mask'].to(embeds_init)\n",
    "            input_lengths = torch.sum(input_mask, 1)\n",
    "            if self.adv_norm_type == \"l2\":\n",
    "                delta = torch.zeros_like(embeds_init).uniform_(-1, 1) * input_mask.unsqueeze(2)\n",
    "                dims = input_lengths * embeds_init.size(-1)\n",
    "                mag = self.adv_init_mag / torch.sqrt(dims)\n",
    "                delta = (delta * mag.view(-1, 1, 1)).detach()\n",
    "            elif self.adv_norm_type == \"linf\":\n",
    "                delta = torch.zeros_like(embeds_init).uniform_(-self.adv_init_mag, self.adv_init_mag)\n",
    "                delta = delta * input_mask.unsqueeze(2)\n",
    "        else:\n",
    "            delta = torch.zeros_like(embeds_init)  # 扰动初始化\n",
    "        loss, logits = None, None\n",
    "        for astep in range(self.adv_K):\n",
    "            delta.requires_grad_()\n",
    "            inputs['inputs_embeds'] = delta + embeds_init  # 累积一次扰动delta\n",
    "            inputs['input_ids'] = None\n",
    "            outputs = model.encoder(**inputs)[0]\n",
    "            logits=outputs # 4*1\n",
    "            prob=F.sigmoid(logits)\n",
    "            \n",
    "            labels=labels.float()\n",
    "            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)\n",
    "            loss=-loss.mean()\n",
    "            \n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            delta_grad = delta.grad.clone().detach()  # 备份扰动的grad\n",
    "            if self.adv_norm_type == \"l2\":\n",
    "                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1)\n",
    "                denorm = torch.clamp(denorm, min=1e-8)\n",
    "                delta = (delta + self.adv_lr * delta_grad / denorm).detach()\n",
    "                if self.adv_max_norm > 0:\n",
    "                    delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()\n",
    "                    exceed_mask = (delta_norm > self.adv_max_norm).to(embeds_init)\n",
    "                    reweights = (self.adv_max_norm / delta_norm * exceed_mask + (1 - exceed_mask)).view(-1, 1, 1)\n",
    "                    delta = (delta * reweights).detach()\n",
    "            elif self.adv_norm_type == \"linf\":\n",
    "                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1, p=float(\"inf\")).view(-1, 1, 1)  # p='inf',无穷范数，获取绝对值最大者\n",
    "                denorm = torch.clamp(denorm, min=1e-8)  # 类似np.clip，将数值夹逼到(min, max)之间\n",
    "                delta = (delta + self.adv_lr * delta_grad / denorm).detach()  # 计算该步的delta，然后累加到原delta值上(梯度上升)\n",
    "                if self.adv_max_norm > 0:\n",
    "                    delta = torch.clamp(delta, -self.adv_max_norm, self.adv_max_norm).detach()\n",
    "            else:\n",
    "                raise ValueError(\"Norm type {} not specified.\".format(self.adv_norm_type))\n",
    "            if isinstance(model, torch.nn.DataParallel):  \n",
    "                embeds_init = model.encoder.roberta.embeddings.word_embeddings(input_ids)\n",
    "            else:\n",
    "                embeds_init = model.encoder.roberta.embeddings.word_embeddings(input_ids)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_adv:\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"bbox\": layout,\n",
    "        \"token_type_ids\": segment_ids,\n",
    "        \"attention_mask\": input_mask,\n",
    "        \"masked_lm_labels\": lm_label_ids\n",
    "    }\n",
    "    loss, prediction_scores = freelb.attack(model, inputs)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "scheduler.step()\n",
    "model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch_cka import CKA\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "from model import Model\n",
    "from run import TextDataset\n",
    "\n",
    "\n",
    "def compare_codebert(name1, name2, data_path, args):\n",
    "    path_dict = {\n",
    "        \"ori\": \"./saved_models/\",\n",
    "        \"adv\": \"./saved_models/FREELB\"\n",
    "    }\n",
    "\n",
    "    name_dict = {\n",
    "        \"ori\": \"CodeBERT\",\n",
    "        \"adv\": \"CodeBERT-FreeLB\",\n",
    "        \"pre\": \"CodeBERT-pre\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(args.model_name_or_path)\n",
    "    config.num_labels = 1\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    model1 = RobertaForSequenceClassification.from_pretrained(args.model_name_or_path, config=config)\n",
    "    model1 = Model(model1, config, tokenizer, None)\n",
    "\n",
    "    if name1 != \"pre\":\n",
    "        model_path1 = path_dict[name1]\n",
    "        checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
    "        output_dir = os.path.join(model_path1, '{}'.format(checkpoint_prefix))\n",
    "        model1.load_state_dict(torch.load(output_dir))\n",
    "\n",
    "\n",
    "    model1.to(args.device)\n",
    "\n",
    "    # for name, param in model1.named_parameters():\n",
    "    #     print(name, param.shape)\n",
    "    #\n",
    "    # exit(0)\n",
    "\n",
    "    model2 = RobertaForSequenceClassification.from_pretrained(args.model_name_or_path, config=config)\n",
    "    model2 = Model(model2, config, tokenizer, None)\n",
    "\n",
    "    if name2 != \"pre\":\n",
    "        model_path2 = path_dict[name2]\n",
    "        checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
    "        output_dir = os.path.join(model_path2, '{}'.format(checkpoint_prefix))\n",
    "        model2.load_state_dict(torch.load(output_dir))\n",
    "    model2.to(args.device)\n",
    "\n",
    "    test_dataset = TextDataset(tokenizer, args, data_path)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8, num_workers=4,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    layers = ['encoder.roberta.embeddings']\n",
    "    for i in range(12):\n",
    "        layer = 'encoder.roberta.encoder.layer.{}.output'.format(i)\n",
    "        layers.append(layer)\n",
    "\n",
    "    # layers.append('encoder.classifier')\n",
    "\n",
    "    cka = CKA(model1, model2,\n",
    "              model1_name=name_dict[name1],  # good idea to provide names to avoid confusion\n",
    "              model2_name=name_dict[name2],\n",
    "              model1_layers=layers,\n",
    "              model2_layers=layers,\n",
    "              device='cuda')\n",
    "    cka.compare(test_dataloader)  # secondary dataloader is optional\n",
    "    if not os.path.exists(\"./cka/\"):\n",
    "        os.makedirs(\"./cka/\")\n",
    "    cka.plot_results(save_path=f\"./cka/Defect_FreeLB_CodeBERT_{name1}_{name2}.png\")\n",
    "    results = cka.export()  # returns a dict that contains model names, layer names and the CKA matrix\n",
    "\n",
    "    for key, value in results.items():\n",
    "        print(key, value)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "## Required parameters\n",
    "parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
    "                    help=\"The model checkpoint for weights initialization.\")\n",
    "\n",
    "parser.add_argument(\"--test_data_file\", default=\"../dataset/test.jsonl\", type=str,\n",
    "                    help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
    "parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "                    help=\"Optional input sequence length after tokenization.\"\n",
    "                         \"The training dataset will be truncated in block of this size for training.\"\n",
    "                         \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.block_size = 512\n",
    "\n",
    "\n",
    "args.model_name_or_path = \"microsoft/codebert-base\"\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args.test_data_file = \"../dataset/test_adv.jsonl\"\n",
    "\n",
    "name1 = \"ori\"\n",
    "name2 = \"FreeLB\"\n",
    "name3 = \"pre\"\n",
    "\n",
    "\n",
    "compare_codebert(name1, name2, args.test_data_file, args)\n",
    "\n",
    "\n",
    "\n",
    "# for name, param in model1.named_parameters():\n",
    "#     print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n",
      "tensor([6144, 6144])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个形状为 [16, 512, 768] 的张量\n",
    "tensor_A = torch.randn(16, 512, 768)\n",
    "tensor_B = torch.randn(2, 8)\n",
    "tensor_C = tensor_B.ne(1)\n",
    "print(tensor_C)\n",
    "input_lengths = torch.sum(tensor_C, 1)\n",
    "# 使用 torch.mul() 进行逐元素相乘\n",
    "# result = torch.einsum(\"abc,abb->abc\",tensor_A,tensor_B)\n",
    "# print(result.shape)\n",
    "print(input_lengths*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 768])\n",
      "torch.Size([16, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个形状为 [16, 512, 768] 的张量\n",
    "tensor_A = torch.randn(16, 512, 768)\n",
    "tensor_B = torch.randn(16, 512, 768)\n",
    "\n",
    "# 使用 torch.mul() 进行逐元素相乘\n",
    "result1 = torch.mul(tensor_A, tensor_B)\n",
    "\n",
    "# 或者直接使用 * 运算符\n",
    "result2 = tensor_A * tensor_B\n",
    "\n",
    "# 打印结果的形状\n",
    "print(result1.shape)  # 输出 torch.Size([16, 512, 768])\n",
    "print(result2.shape)  # 输出 torch.Size([16, 512, 768])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ljx_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
